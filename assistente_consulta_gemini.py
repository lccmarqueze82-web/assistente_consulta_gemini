import streamlit as st
from google import genai # Importar a biblioteca do Google GenAI
from google.genai.errors import APIError

st.set_page_config(page_title="Assistente de Consulta Gemini", layout="wide")

st.title("ü©∫ Assistente de Consulta Gemini")

# Inicializa o cliente Gemini
# A chave de API deve ser configurada em .streamlit/secrets.toml como
# GOOGLE_API_KEY="SUA_CHAVE_AQUI"
try:
    client = genai.Client(api_key=st.secrets["GOOGLE_API_KEY"])
except KeyError:
    st.error("ERRO: Chave 'GOOGLE_API_KEY' n√£o encontrada nos segredos do Streamlit. Por favor, adicione sua chave de API do Gemini em .streamlit/secrets.toml.")
    st.stop()
except Exception as e:
    st.error(f"ERRO ao inicializar o cliente Gemini: {e}")
    st.stop()

# Usaremos o modelo 'gemini-2.5-flash' ou outro modelo de chat/texto adequado.
GEMINI_MODEL = "gemini-2.5-flash" 

st.markdown("""
O assistente trabalha em 4 etapas:
1Ô∏è‚É£ **Caixa 1** ‚Äì Informa√ß√£o crua  
2Ô∏è‚É£ **Caixa 2** ‚Äì Aplica Prompt PEC1 atualizado  
3Ô∏è‚É£ **Caixa 3** ‚Äì Sugest√µes e condutas  
4Ô∏è‚É£ **Caixa 4** ‚Äì Chat livre com Gemini  
---
""")

# --- Layout das Caixas de Texto ---
col1, col2, col3 = st.columns(3)

with col1:
    caixa1 = st.text_area("CAIXA 1 - Informa√ß√£o Crua", height=250, key="caixa1")

with col2:
    caixa2 = st.text_area("CAIXA 2 - Prompt PEC1 Atualizado", height=250, key="caixa2")

with col3:
    caixa3 = st.text_area("CAIXA 3 - Sugest√µes e Discuss√£o", height=250, key="caixa3")

caixa4 = st.text_input("CAIXA 4 - Chat com Gemini", key="caixa4")

# --- Layout dos Bot√µes ---
colA, colB, colC = st.columns([1, 1, 2])

with colA:
    if st.button("üßπ LIMPAR"):
        for key in ["caixa1","caixa2","caixa3","caixa4"]:
            # Limpa o valor das chaves no session_state
            st.session_state[key] = ""
        st.rerun()

with colB:
    if st.button("üìã COPIAR CAIXA 2"):
        st.write("Conte√∫do da Caixa 2 copiado (copie manualmente abaixo):")
        # st.code exibe o conte√∫do para que o usu√°rio possa copiar manualmente
        st.code(st.session_state.get("caixa2", "")) 

with colC:
    aplicar = st.button("‚öôÔ∏è Aplicar Prompt PEC1")

# --- Fun√ß√£o de Chamada do Gemini ---
def gemini_reply(system_instruction, text_input):
    """Fun√ß√£o para chamar o modelo Gemini com instru√ß√µes de sistema."""
    
    # O SDK do Gemini usa 'system_instruction' no par√¢metro 'config'
    config = genai.types.GenerateContentConfig(
        system_instruction=system_instruction
    )
    
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=text_input, # O conte√∫do a ser processado pelo modelo
            config=config 
        )
        return response.text.strip()
    except APIError as e:
        st.error(f"Erro da API do Gemini: {e}")
        return f"ERRO NA API: {e}"
    except Exception as e:
        st.error(f"Erro inesperado: {e}")
        return f"ERRO INESPERADO: {e}"

# --- Etapa 2: Aplicar Prompt PEC1 ---
if aplicar and caixa1:
    with st.spinner("Aplicando Prompt PEC1..."):
        # Role do sistema para a Etapa 2
        system_role_pec1 = "Voc√™ √© um assistente de processamento de texto. Sua tarefa √© aplicar o 'Prompt PEC1 atualizado' ao texto de entrada, formatando e estruturando-o conforme as diretrizes do PEC1."
        
        st.session_state["caixa2"] = gemini_reply(
            system_role_pec1,
            caixa1
        )
        st.success("‚úÖ Prompt aplicado!")

# --- Etapa 3: Gerar Sugest√µes ---
# S√≥ exibe o bot√£o se a Caixa 2 tiver conte√∫do
if st.session_state.get("caixa2"):
    if st.button("üí¨ Gerar Sugest√µes (Caixa 3)"):
        with st.spinner("Analisando diagn√≥stico..."):
            # Role do sistema para a Etapa 3
            system_role_sugestoes = "Voc√™ √© um assistente m√©dico de IA. Analise cuidadosamente o texto processado, que j√° est√° formatado com o Prompt PEC1, e gere sugest√µes de diagn√≥sticos diferenciais e condutas m√©dicas apropriadas. Seja claro, conciso e use linguagem m√©dica profissional."
            
            st.session_state["caixa3"] = gemini_reply(
                system_role_sugestoes,
                st.session_state["caixa2"]
            )
            st.success("‚úÖ Sugest√µes geradas!")

# --- Etapa 4: Chat Livre ---
if caixa4:
    if st.button("üí≠ Enviar Chat (Caixa 4)"):
        with st.spinner("Respondendo..."):
            # Role do sistema para a Etapa 4
            system_role_chat = "Voc√™ √© um assistente de chat geral e prestativo. Responda √† pergunta do usu√°rio. Mantenha o contexto de ser um assistente, mas responda de forma livre."
            
            resposta = gemini_reply(system_role_chat, caixa4)
            st.markdown(f"**Gemini:** {resposta}")

# --- ANOTA√á√ïES HIST√ìRICAS (MANTIDAS COMO COMENT√ÅRIOS) ---
# O bloco de c√≥digo abaixo estava causando um erro de sintaxe (duplica√ß√£o de fun√ß√£o e texto solto).
# Ele foi removido e a fun√ß√£o gemini_reply original (definida acima) foi mantida.
#
# # Modelo:
# # Defini um modelo Gemini: GEMINI_MODEL = "gemini-2.5-flash". Voc√™ pode usar outro modelo, como gemini-2.5-pro, se precisar de racioc√≠nio mais complexo.
# # Fun√ß√£o de Chamada (gemini_reply):
# # Substitu√≠mos a chamada client.chat.completions.create(...) pela chamada client.models.generate_content(...).
# # O papel do sistema (o role: "system") no Gemini √© passado atrav√©s do par√¢metro config como system_instruction. Isso √© crucial para as etapas 2 e 3.
#
# # A redefini√ß√£o de gemini_reply abaixo foi removida:
# # def gemini_reply(system_instruction, text_input):
# #     config = genai.types.GenerateContentConfig(
# #         system_instruction=system_instruction
# #     )
# #     response = client.models.generate_content(
# #         model=GEMINI_MODEL,
# #         contents=text_input,
# #         config=config 
# #     )
# #     return response.text.strip()
